{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f04afd6-f0cb-4ea7-b33b-7ccdd73dcbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/huggingface/diffusers/\n",
    "# this implementation works on diffusers >= 0.8.0\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from torch import autocast, inference_mode\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd21273b-76a9-4b61-8094-431e33f42830",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "from diffusers.configuration_utils import ConfigMixin, register_to_config\n",
    "from diffusers.utils import BaseOutput, deprecate\n",
    "from diffusers.schedulers.scheduling_utils import SchedulerMixin\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DDIMSchedulerOutput(BaseOutput):\n",
    "    \"\"\"\n",
    "    Output class for the scheduler's step function output.\n",
    "\n",
    "    Args:\n",
    "        prev_sample (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)` for images):\n",
    "            Computed sample (x_{t-1}) of previous timestep. `prev_sample` should be used as next model input in the\n",
    "            denoising loop.\n",
    "        next_sample (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)` for images):\n",
    "            Computed sample (x_{t+1}) of previous timestep. `next_sample` should be used as next model input in the\n",
    "            reverse denoising loop.\n",
    "        pred_original_sample (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)` for images):\n",
    "            The predicted denoised sample (x_{0}) based on the model output from the current timestep.\n",
    "            `pred_original_sample` can be used to preview progress or for guidance.\n",
    "    \"\"\"\n",
    "\n",
    "    prev_sample: Optional[torch.FloatTensor] = None\n",
    "    next_sample: Optional[torch.FloatTensor] = None\n",
    "    pred_original_sample: Optional[torch.FloatTensor] = None\n",
    "\n",
    "\n",
    "def betas_for_alpha_bar(num_diffusion_timesteps, max_beta=0.999) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Create a beta schedule that discretizes the given alpha_t_bar function, which defines the cumulative product of\n",
    "    (1-beta) over time from t = [0,1].\n",
    "\n",
    "    Contains a function alpha_bar that takes an argument t and transforms it to the cumulative product of (1-beta) up\n",
    "    to that part of the diffusion process.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        num_diffusion_timesteps (`int`): the number of betas to produce.\n",
    "        max_beta (`float`): the maximum beta to use; use values lower than 1 to\n",
    "                     prevent singularities.\n",
    "\n",
    "    Returns:\n",
    "        betas (`np.ndarray`): the betas used by the scheduler to step the model outputs\n",
    "    \"\"\"\n",
    "\n",
    "    def alpha_bar(time_step):\n",
    "        return math.cos((time_step + 0.008) / 1.008 * math.pi / 2) ** 2\n",
    "\n",
    "    betas = []\n",
    "    for i in range(num_diffusion_timesteps):\n",
    "        t1 = i / num_diffusion_timesteps\n",
    "        t2 = (i + 1) / num_diffusion_timesteps\n",
    "        betas.append(min(1 - alpha_bar(t2) / alpha_bar(t1), max_beta))\n",
    "    return torch.tensor(betas)\n",
    "\n",
    "\n",
    "class DDIMScheduler(SchedulerMixin, ConfigMixin):\n",
    "    \"\"\"\n",
    "    Denoising diffusion implicit models is a scheduler that extends the denoising procedure introduced in denoising\n",
    "    diffusion probabilistic models (DDPMs) with non-Markovian guidance.\n",
    "\n",
    "    [`~ConfigMixin`] takes care of storing all config attributes that are passed in the scheduler's `__init__`\n",
    "    function, such as `num_train_timesteps`. They can be accessed via `scheduler.config.num_train_timesteps`.\n",
    "    [`~ConfigMixin`] also provides general loading and saving functionality via the [`~ConfigMixin.save_config`] and\n",
    "    [`~ConfigMixin.from_config`] functions.\n",
    "\n",
    "    For more details, see the original paper: https://arxiv.org/abs/2010.02502\n",
    "\n",
    "    Args:\n",
    "        num_train_timesteps (`int`): number of diffusion steps used to train the model.\n",
    "        beta_start (`float`): the starting `beta` value of inference.\n",
    "        beta_end (`float`): the final `beta` value.\n",
    "        beta_schedule (`str`):\n",
    "            the beta schedule, a mapping from a beta range to a sequence of betas for stepping the model. Choose from\n",
    "            `linear`, `scaled_linear`, or `squaredcos_cap_v2`.\n",
    "        trained_betas (`np.ndarray`, optional):\n",
    "            option to pass an array of betas directly to the constructor to bypass `beta_start`, `beta_end` etc.\n",
    "        clip_sample (`bool`, default `True`):\n",
    "            option to clip predicted sample between -1 and 1 for numerical stability.\n",
    "        set_alpha_to_one (`bool`, default `True`):\n",
    "            each diffusion step uses the value of alphas product at that step and at the previous one. For the final\n",
    "            step there is no previous alpha. When this option is `True` the previous alpha product is fixed to `1`,\n",
    "            otherwise it uses the value of alpha at step 0.\n",
    "        steps_offset (`int`, default `0`):\n",
    "            an offset added to the inference steps. You can use a combination of `offset=1` and\n",
    "            `set_alpha_to_one=False`, to make the last step use step 0 for the previous alpha product, as done in\n",
    "            stable diffusion.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    @register_to_config\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_train_timesteps: int = 1000,\n",
    "        beta_start: float = 0.0001,\n",
    "        beta_end: float = 0.02,\n",
    "        beta_schedule: str = \"linear\",\n",
    "        trained_betas: Optional[np.ndarray] = None,\n",
    "        clip_sample: bool = True,\n",
    "        set_alpha_to_one: bool = True,\n",
    "        steps_offset: int = 0,\n",
    "    ):\n",
    "        if trained_betas is not None:\n",
    "            self.betas = torch.from_numpy(trained_betas)\n",
    "        elif beta_schedule == \"linear\":\n",
    "            self.betas = torch.linspace(beta_start, beta_end, num_train_timesteps, dtype=torch.float32)\n",
    "        elif beta_schedule == \"scaled_linear\":\n",
    "            # this schedule is very specific to the latent diffusion model.\n",
    "            self.betas = (\n",
    "                torch.linspace(beta_start**0.5, beta_end**0.5, num_train_timesteps, dtype=torch.float32) ** 2\n",
    "            )\n",
    "        elif beta_schedule == \"squaredcos_cap_v2\":\n",
    "            # Glide cosine schedule\n",
    "            self.betas = betas_for_alpha_bar(num_train_timesteps)\n",
    "        else:\n",
    "            raise NotImplementedError(f\"{beta_schedule} does is not implemented for {self.__class__}\")\n",
    "\n",
    "        self.alphas = 1.0 - self.betas\n",
    "        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n",
    "\n",
    "        # At every step in ddim, we are looking into the previous alphas_cumprod\n",
    "        # For the final step, there is no previous alphas_cumprod because we are already at 0\n",
    "        # `set_alpha_to_one` decides whether we set this parameter simply to one or\n",
    "        # whether we use the final alpha of the \"non-previous\" one.\n",
    "        self.final_alpha_cumprod = torch.tensor(1.0) if set_alpha_to_one else self.alphas_cumprod[0]\n",
    "\n",
    "        # standard deviation of the initial noise distribution\n",
    "        self.init_noise_sigma = 1.0\n",
    "\n",
    "        # setable values\n",
    "        self.num_inference_steps = None\n",
    "        self.timesteps = torch.from_numpy(np.arange(0, num_train_timesteps)[::-1].copy().astype(np.int64))\n",
    "\n",
    "    def _get_variance(self, timestep, prev_timestep):\n",
    "        alpha_prod_t = self.alphas_cumprod[timestep]\n",
    "        alpha_prod_t_prev = self.alphas_cumprod[prev_timestep] if prev_timestep >= 0 else self.final_alpha_cumprod\n",
    "        beta_prod_t = 1 - alpha_prod_t\n",
    "        beta_prod_t_prev = 1 - alpha_prod_t_prev\n",
    "\n",
    "        variance = (beta_prod_t_prev / beta_prod_t) * (1 - alpha_prod_t / alpha_prod_t_prev)\n",
    "\n",
    "        return variance\n",
    "\n",
    "    def set_timesteps(self, num_inference_steps: int, device: Union[str, torch.device] = None):\n",
    "        \"\"\"\n",
    "        Sets the discrete timesteps used for the diffusion chain. Supporting function to be run before inference.\n",
    "        Args:\n",
    "            num_inference_steps (`int`):\n",
    "                the number of diffusion steps used when generating samples with a pre-trained model.\n",
    "        \"\"\"\n",
    "        self.num_inference_steps = num_inference_steps\n",
    "        step_ratio = self.config.num_train_timesteps // self.num_inference_steps\n",
    "        # creates integer timesteps by multiplying by ratio\n",
    "        # casting to int to avoid issues when num_inference_step is power of 3\n",
    "        timesteps = (np.arange(0, num_inference_steps) * step_ratio).round()[::-1].copy().astype(np.int64)\n",
    "        self.timesteps = torch.from_numpy(timesteps).to(device)\n",
    "        self.timesteps += self.config.steps_offset\n",
    "        \n",
    "    def scale_model_input(self, sample: torch.FloatTensor, timestep: Optional[int] = None) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        Ensures interchangeability with schedulers that need to scale the denoising model input depending on the\n",
    "        current timestep.\n",
    "        Args:\n",
    "            sample (`torch.FloatTensor`): input sample\n",
    "            timestep (`int`, optional): current timestep\n",
    "        Returns:\n",
    "            `torch.FloatTensor`: scaled input sample\n",
    "        \"\"\"\n",
    "        return sample\n",
    "\n",
    "    def step(\n",
    "        self,\n",
    "        model_output: torch.FloatTensor,\n",
    "        timestep: int,\n",
    "        sample: torch.FloatTensor,\n",
    "        eta: float = 0.0,\n",
    "        use_clipped_model_output: bool = False,\n",
    "        generator=None,\n",
    "        return_dict: bool = True,\n",
    "    ) -> Union[DDIMSchedulerOutput, Tuple]:\n",
    "        \"\"\"\n",
    "        Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion\n",
    "        process from the learned model outputs (most often the predicted noise).\n",
    "\n",
    "        Args:\n",
    "            model_output (`torch.FloatTensor`): direct output from learned diffusion model.\n",
    "            timestep (`int`): current discrete timestep in the diffusion chain.\n",
    "            sample (`torch.FloatTensor`):\n",
    "                current instance of sample being created by diffusion process.\n",
    "            eta (`float`): weight of noise for added noise in diffusion step.\n",
    "            use_clipped_model_output (`bool`): TODO\n",
    "            generator: random number generator.\n",
    "            return_dict (`bool`): option for returning tuple rather than DDIMSchedulerOutput class\n",
    "\n",
    "        Returns:\n",
    "            [`~schedulers.scheduling_utils.DDIMSchedulerOutput`] or `tuple`:\n",
    "            [`~schedulers.scheduling_utils.DDIMSchedulerOutput`] if `return_dict` is True, otherwise a `tuple`. When\n",
    "            returning a tuple, the first element is the sample tensor.\n",
    "\n",
    "        \"\"\"\n",
    "        if self.num_inference_steps is None:\n",
    "            raise ValueError(\n",
    "                \"Number of inference steps is 'None', you need to run 'set_timesteps' after creating the scheduler\"\n",
    "            )\n",
    "\n",
    "        # See formulas (12) and (16) of DDIM paper https://arxiv.org/pdf/2010.02502.pdf\n",
    "        # Ideally, read DDIM paper in-detail understanding\n",
    "\n",
    "        # Notation (<variable name> -> <name in paper>\n",
    "        # - pred_noise_t -> e_theta(x_t, t)\n",
    "        # - pred_original_sample -> f_theta(x_t, t) or x_0\n",
    "        # - std_dev_t -> sigma_t\n",
    "        # - eta -> η\n",
    "        # - pred_sample_direction -> \"direction pointing to x_t\"\n",
    "        # - pred_prev_sample -> \"x_t-1\"\n",
    "\n",
    "        # 1. get previous step value (=t-1)\n",
    "        prev_timestep = timestep - self.config.num_train_timesteps // self.num_inference_steps\n",
    "\n",
    "        # 2. compute alphas, betas\n",
    "        alpha_prod_t = self.alphas_cumprod[timestep]\n",
    "        alpha_prod_t_prev = self.alphas_cumprod[prev_timestep] if prev_timestep >= 0 else self.final_alpha_cumprod\n",
    "\n",
    "        beta_prod_t = 1 - alpha_prod_t\n",
    "\n",
    "        # 3. compute predicted original sample from predicted noise also called\n",
    "        # \"predicted x_0\" of formula (12) from https://arxiv.org/pdf/2010.02502.pdf\n",
    "        pred_original_sample = (sample - beta_prod_t ** (0.5) * model_output) / alpha_prod_t ** (0.5)\n",
    "\n",
    "        # 4. Clip \"predicted x_0\"\n",
    "        if self.config.clip_sample:\n",
    "            pred_original_sample = torch.clamp(pred_original_sample, -1, 1)\n",
    "\n",
    "        # 5. compute variance: \"sigma_t(η)\" -> see formula (16)\n",
    "        # σ_t = sqrt((1 − α_t−1)/(1 − α_t)) * sqrt(1 − α_t/α_t−1)\n",
    "        variance = self._get_variance(timestep, prev_timestep)\n",
    "        std_dev_t = eta * variance ** (0.5)\n",
    "\n",
    "        if use_clipped_model_output:\n",
    "            # the model_output is always re-derived from the clipped x_0 in Glide\n",
    "            model_output = (sample - alpha_prod_t ** (0.5) * pred_original_sample) / beta_prod_t ** (0.5)\n",
    "\n",
    "        # 6. compute \"direction pointing to x_t\" of formula (12) from https://arxiv.org/pdf/2010.02502.pdf\n",
    "        pred_sample_direction = (1 - alpha_prod_t_prev - std_dev_t**2) ** (0.5) * model_output\n",
    "\n",
    "        # 7. compute x_t without \"random noise\" of formula (12) from https://arxiv.org/pdf/2010.02502.pdf\n",
    "        prev_sample = alpha_prod_t_prev ** (0.5) * pred_original_sample + pred_sample_direction\n",
    "\n",
    "        if eta > 0:\n",
    "            device = model_output.device if torch.is_tensor(model_output) else \"cpu\"\n",
    "            noise = torch.randn(model_output.shape, generator=generator).to(device)\n",
    "            variance = self._get_variance(timestep, prev_timestep) ** (0.5) * eta * noise\n",
    "\n",
    "            prev_sample = prev_sample + variance\n",
    "\n",
    "        if not return_dict:\n",
    "            return (prev_sample,)\n",
    "\n",
    "        return DDIMSchedulerOutput(prev_sample=prev_sample, pred_original_sample=pred_original_sample)\n",
    "\n",
    "    def reverse_step(\n",
    "        self,\n",
    "        model_output: torch.FloatTensor,\n",
    "        timestep: int,\n",
    "        sample: torch.FloatTensor,\n",
    "        eta: float = 0.0,\n",
    "        use_clipped_model_output: bool = False,\n",
    "        generator=None,\n",
    "        return_dict: bool = True,\n",
    "    ) -> Union[DDIMSchedulerOutput, Tuple]:\n",
    "        \"\"\"\n",
    "        Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion\n",
    "        process from the learned model outputs (most often the predicted noise).\n",
    "\n",
    "        Args:\n",
    "            model_output (`torch.FloatTensor`): direct output from learned diffusion model.\n",
    "            timestep (`int`): current discrete timestep in the diffusion chain.\n",
    "            sample (`torch.FloatTensor`):\n",
    "                current instance of sample being created by diffusion process.\n",
    "            eta (`float`): weight of noise for added noise in diffusion step.\n",
    "            use_clipped_model_output (`bool`): TODO\n",
    "            generator: random number generator.\n",
    "            return_dict (`bool`): option for returning tuple rather than DDIMSchedulerOutput class\n",
    "\n",
    "        Returns:\n",
    "            [`~schedulers.scheduling_utils.DDIMSchedulerOutput`] or `tuple`:\n",
    "            [`~schedulers.scheduling_utils.DDIMSchedulerOutput`] if `return_dict` is True, otherwise a `tuple`. When\n",
    "            returning a tuple, the first element is the sample tensor.\n",
    "\n",
    "        \"\"\"\n",
    "        if self.num_inference_steps is None:\n",
    "            raise ValueError(\n",
    "                \"Number of inference steps is 'None', you need to run 'set_timesteps' after creating the scheduler\"\n",
    "            )\n",
    "\n",
    "        # See formulas (12) and (16) of DDIM paper https://arxiv.org/pdf/2010.02502.pdf\n",
    "        # Ideally, read DDIM paper in-detail understanding\n",
    "\n",
    "        # Notation (<variable name> -> <name in paper>\n",
    "        # - pred_noise_t -> e_theta(x_t, t)\n",
    "        # - pred_original_sample -> f_theta(x_t, t) or x_0\n",
    "        # - std_dev_t -> sigma_t\n",
    "        # - eta -> η\n",
    "        # - pred_sample_direction -> \"direction pointing to x_t\"\n",
    "        # - pred_prev_sample -> \"x_t-1\"\n",
    "\n",
    "        # 1. get previous step value (=t-1)\n",
    "        next_timestep = min(self.config.num_train_timesteps - 2,\n",
    "                            timestep + self.config.num_train_timesteps // self.num_inference_steps)\n",
    "\n",
    "        # 2. compute alphas, betas\n",
    "        alpha_prod_t = self.alphas_cumprod[timestep]\n",
    "        alpha_prod_t_next = self.alphas_cumprod[next_timestep] if next_timestep >= 0 else self.final_alpha_cumprod\n",
    "\n",
    "        beta_prod_t = 1 - alpha_prod_t\n",
    "\n",
    "        # 3. compute predicted original sample from predicted noise also called\n",
    "        # \"predicted x_0\" of formula (12) from https://arxiv.org/pdf/2010.02502.pdf\n",
    "        pred_original_sample = (sample - beta_prod_t ** (0.5) * model_output) / alpha_prod_t ** (0.5)\n",
    "\n",
    "        # 4. Clip \"predicted x_0\"\n",
    "        if self.config.clip_sample:\n",
    "            pred_original_sample = torch.clamp(pred_original_sample, -1, 1)\n",
    "\n",
    "        # 5. TODO: simple noising implementatiom\n",
    "        next_sample = self.add_noise(pred_original_sample,\n",
    "                                     model_output,\n",
    "                                     torch.LongTensor([next_timestep]))\n",
    "\n",
    "        # # 5. compute variance: \"sigma_t(η)\" -> see formula (16)\n",
    "        # # σ_t = sqrt((1 − α_t−1)/(1 − α_t)) * sqrt(1 − α_t/α_t−1)\n",
    "        # variance = self._get_variance(next_timestep, timestep)\n",
    "        # std_dev_t = eta * variance ** (0.5)\n",
    "\n",
    "        # if use_clipped_model_output:\n",
    "        #     # the model_output is always re-derived from the clipped x_0 in Glide\n",
    "        #     model_output = (sample - alpha_prod_t ** (0.5) * pred_original_sample) / beta_prod_t ** (0.5)\n",
    "\n",
    "        # # 6. compute \"direction pointing to x_t\" of formula (12) from https://arxiv.org/pdf/2010.02502.pdf\n",
    "        # pred_sample_direction = (1 - alpha_prod_t_next - std_dev_t**2) ** (0.5) * model_output\n",
    "\n",
    "        # # 7. compute x_t without \"random noise\" of formula (12) from https://arxiv.org/pdf/2010.02502.pdf\n",
    "        # next_sample = alpha_prod_t_next ** (0.5) * pred_original_sample + pred_sample_direction\n",
    "\n",
    "        if not return_dict:\n",
    "            return (next_sample,)\n",
    "\n",
    "        return DDIMSchedulerOutput(next_sample=next_sample, pred_original_sample=pred_original_sample)\n",
    "\n",
    "    def add_noise(\n",
    "        self,\n",
    "        original_samples: torch.FloatTensor,\n",
    "        noise: torch.FloatTensor,\n",
    "        timesteps: torch.IntTensor,\n",
    "    ) -> torch.FloatTensor:\n",
    "        if self.alphas_cumprod.device != original_samples.device:\n",
    "            self.alphas_cumprod = self.alphas_cumprod.to(original_samples.device)\n",
    "        if timesteps.device != original_samples.device:\n",
    "            timesteps = timesteps.to(original_samples.device)\n",
    "\n",
    "        sqrt_alpha_prod = self.alphas_cumprod[timesteps] ** 0.5\n",
    "        sqrt_alpha_prod = sqrt_alpha_prod.flatten()\n",
    "        while len(sqrt_alpha_prod.shape) < len(original_samples.shape):\n",
    "            sqrt_alpha_prod = sqrt_alpha_prod.unsqueeze(-1)\n",
    "\n",
    "        sqrt_one_minus_alpha_prod = (1 - self.alphas_cumprod[timesteps]) ** 0.5\n",
    "        sqrt_one_minus_alpha_prod = sqrt_one_minus_alpha_prod.flatten()\n",
    "        while len(sqrt_one_minus_alpha_prod.shape) < len(original_samples.shape):\n",
    "            sqrt_one_minus_alpha_prod = sqrt_one_minus_alpha_prod.unsqueeze(-1)\n",
    "\n",
    "        noisy_samples = sqrt_alpha_prod * original_samples + sqrt_one_minus_alpha_prod * noise\n",
    "        return noisy_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.config.num_train_timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3fa09ea-1ef8-4fbc-8f11-e04ab851ec4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'use_auth_token': '<YOUR-HF-TOKEN>'} are not expected by StableDiffusionPipeline and will be ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f77cc0e8c90e4fefa421ad546226e4cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "# don't forget to add your token or comment if already logged in\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", \n",
    "                                               scheduler=DDIMScheduler(beta_end=0.012,\n",
    "                                                                       beta_schedule=\"scaled_linear\",\n",
    "                                                                       beta_start=0.00085),\n",
    "                                               use_auth_token=\"<YOUR-HF-TOKEN>\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47a8b36e-e5f9-4f02-b1c6-6d6da8bcbc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_lat(latents):\n",
    "    # utility function for visualization of diffusion process\n",
    "    with torch.no_grad():\n",
    "        images = pipe.decode_latents(latents)\n",
    "        im = pipe.numpy_to_pil(images)[0].resize((128, 128))\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee47dc8e-e063-4ca0-a50b-935c63585b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image):\n",
    "    w, h = image.size\n",
    "    w, h = map(lambda x: x - x % 32, (w, h))  # resize to integer multiple of 32\n",
    "    image = image.resize((w, h), resample=Image.LANCZOS)\n",
    "    image = np.array(image).astype(np.float32) / 255.0\n",
    "    image = image[None].transpose(0, 3, 1, 2)\n",
    "    image = torch.from_numpy(image)\n",
    "    return 2.0 * image - 1.0\n",
    "\n",
    "def im2latent(pipe, im, generator):\n",
    "    init_image = preprocess(im).to(pipe.device)\n",
    "    init_latent_dist = pipe.vae.encode(init_image).latent_dist\n",
    "    init_latents = init_latent_dist.sample(generator=generator)\n",
    "    \n",
    "    return init_latents * 0.18215"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83efbe74-c28c-4cac-9c12-f1dd96416ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "import utils\n",
    "import importlib\n",
    "#importlib.reload(utils) \n",
    "\n",
    "benchmark = list(utils.load_benchmark_pie_bench([\"000000000007\"]).values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d9877895-7823-40f9-9399-2eaea8ddff70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e0aaf8f-905b-40bd-b124-b849b57652bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mdnikolaev/.conda/envs/rudenkovInfEdit/lib/python3.9/site-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py:291: FutureWarning: `_encode_prompt()` is deprecated and it will be removed in a future version. Use `encode_prompt()` instead. Also, be aware that the output format changed from a concatenated tensor to a tuple.\n",
      "  deprecate(\"_encode_prompt()\", \"1.0.0\", deprecation_message, standard_warn=False)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# use text describing an image\u001b[39;00m\n\u001b[1;32m     10\u001b[0m source_prompt \u001b[38;5;241m=\u001b[39m benchmark[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moriginal_prompt\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 11\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m20\u001b[39m,\u001b[38;5;241m8\u001b[39m))\n\u001b[1;32m     14\u001b[0m decoded_latents \u001b[38;5;241m=\u001b[39m image_latents\u001b[38;5;241m.\u001b[39mclone()\n",
      "File \u001b[0;32m~/.conda/envs/rudenkovInfEdit/lib/python3.9/site-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py:306\u001b[0m, in \u001b[0;36mStableDiffusionPipeline._encode_prompt\u001b[0;34m(self, prompt, device, num_images_per_prompt, do_classifier_free_guidance, negative_prompt, prompt_embeds, negative_prompt_embeds, lora_scale, **kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m prompt_embeds_tuple \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_prompt(\n\u001b[1;32m    294\u001b[0m     prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[1;32m    295\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    303\u001b[0m )\n\u001b[1;32m    305\u001b[0m \u001b[38;5;66;03m# concatenate for backwards comp\u001b[39;00m\n\u001b[0;32m--> 306\u001b[0m prompt_embeds \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mprompt_embeds_tuple\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_embeds_tuple\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m prompt_embeds\n",
      "\u001b[0;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got NoneType"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "# photo from ffhq\n",
    "init_image = Image.open(benchmark[0][\"image_path\"]).resize((512,512))\n",
    "# fix seed\n",
    "g = torch.Generator(device=pipe.device).manual_seed(84)\n",
    "\n",
    "image_latents = im2latent(pipe, init_image, g)\n",
    "pipe.scheduler.set_timesteps(51)\n",
    "# use text describing an image\n",
    "source_prompt = benchmark[0][\"original_prompt\"]\n",
    "context = pipe._encode_prompt(source_prompt, pipe.device, 1, False, \"\")\n",
    "\n",
    "plt.figure(figsize=(20,8))\n",
    "decoded_latents = image_latents.clone()\n",
    "with autocast(\"cuda\"), inference_mode():\n",
    "    # we are pivoting timesteps as we are moving in opposite direction\n",
    "    timesteps = pipe.scheduler.timesteps.flip(0)\n",
    "    # this would be our targets for pivoting\n",
    "    init_trajectory = torch.empty(len(timesteps), *decoded_latents.size()[1:], device=decoded_latents.device, dtype=decoded_latents.dtype)\n",
    "    for i, t in enumerate(tqdm(timesteps)):\n",
    "        init_trajectory[i:i+1] = decoded_latents\n",
    "        noise_pred = pipe.unet(decoded_latents, t, encoder_hidden_states=context).sample\n",
    "        decoded_latents = pipe.scheduler.reverse_step(noise_pred, t, decoded_latents).next_sample\n",
    "        if i % 10 == 0:\n",
    "            plt.subplot(1,6,i//10+1)\n",
    "            plt.imshow(show_lat(decoded_latents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4460017-4ed9-454b-a0d3-7fbeb3abcba3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rudenkovInfEdit)",
   "language": "python",
   "name": "rudenkovinfedit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
